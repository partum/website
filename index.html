<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <title>Jon Barron</title>

    <meta name="author" content="Jon Barron" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
    <link rel="icon" type="image/png" href="images/seal_icon.png" />
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link
      href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;700;900&display=swap"
      rel="stylesheet"
    />
  </head>

  <body>
    <div id="profile">
      <img alt="photo of Jon" src="images/JonBarron_circle.jpg" id="JonPic" />
      <h1>Jon Barron</h1>
      <h2>
        I am a staff research scientist at
        <a href="https://ai.google/research">Google Research</a>, where I work
        on computer vision and machine learning.
      </h2>

      <p class="close">
        At Google I've worked on
        <a
          href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html"
          >Portrait Light</a
        >,
        <a
          href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html"
          >Lens Blur</a
        >,
        <a
          href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html"
          >HDR+</a
        >, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>,
        <a
          href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html"
          >Portrait Mode</a
        >, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>.
      </p>
      <p class="close">
        I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>,
        where I was advised by
        <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and
        funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>.
      </p>
      <p class="close">
        I did my bachelors at the
        <a href="http://cs.toronto.edu">University of Toronto</a>.
      </p>
      <p class="close">
        I've received the
        <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/"
          >C.V. Ramamoorthy Distinguished Research Award</a
        >
        and the
        <a href="https://www.thecvf.com/?page_id=413#YRA"
          >PAMI Young Researcher Award</a
        >.
      </p>
      <p>
        <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp/&nbsp
        <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
        <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
        <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ"
          >Google Scholar</a
        >
        &nbsp/&nbsp
        <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp
        <a href="https://github.com/jonbarron/">Github</a>
      </p>
    </div>

    <div>
      <button onclick="topFunction()" id="myBtn" title="Go to top">^</button> 
      <h2 id="research-link">Research</h2>
      <p>
        I'm interested in computer vision, machine learning, optimization, and
        image processing. Much of my research is about inferring the physical
        world (shape, motion, color, light, etc) from images. Representative
        papers are
        <span class="highlight">highlighted</span>.
      </p>

      <div id="research">
        
        
          <article>
            <div class="one">
              <div class="two" id="ibrnet_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                <video width="100%" height="100%" muted autoplay loop>
                  <source src="images/ibrnet_after.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                </video>
              </div>
              <img src="images/ibrnet_before.jpg" width="160" />
            </div>

            <div class="article-info">     
              <a href="https://ibrnet.github.io/">
                <papertitle
                  >IBRNet: Learning Multi-View Image-Based Rendering</papertitle
                >
              </a>
              <div>
                <a href="https://www.cs.cornell.edu/~qqw/">Qianqian Wang</a>,
                <a href="https://www.linkedin.com/in/zhicheng-wang-96116897/"
                  >Zhicheng Wang</a
                >, <a href="https://www.kylegenova.com/">Kyle Genova</a>,
                <a href="https://people.eecs.berkeley.edu/~pratul/"
                >Pratul Srinivasan</a
                >,
                <a
                href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en"
                >Howard Zhou</a
                >, 
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.ricardomartinbrualla.com/"
                >Ricardo Martin-Brualla</a
                >,
                <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>,
                <a href="https://www.cs.princeton.edu/~funk/"
                >Thomas Funkhouser</a
                >
              </div>
              <div><em>CVPR</em>, 2021</div>
              <a href="https://ibrnet.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2102.13090">arXiv</a>
              <p>
                By learning how to pay attention to input images at render time,
                we can amortize inference for view synthesis and reduce error
                rates by 15%.
              </p>
            </div>
          </article>

          <article>
          
            <div class="one">
                <div class="two" id="nerv_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/hotdog.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/hotdog.jpg" width="160" />
              </div>
              
            <div class="article-info">
              <a href="https://people.eecs.berkeley.edu/~pratul/nerv/">
                <papertitle
                  >NeRV: Neural Reflectione and Visibility Fields for Relighting
                  and View Synthesis</papertitle
                >
              </a>
               <div>
              <a href="https://people.eecs.berkeley.edu/~pratul/"
                >Pratul Srinivasan</a
              >, <a href="https://boyangdeng.com/">Boyang Deng</a>,
              <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
               
              <a href="http://matthewtancik.com/">Matthew Tancik</a>,
              <a href="https://people.eecs.berkeley.edu/~bmild/"
                >Ben Mildenhall</a
              >,
              <strong>Jonathan T. Barron</strong>
            </div>
            <div>
              <em>CVPR</em>, 2021</div>
               
              <a href="https://people.eecs.berkeley.edu/~pratul/nerv/"
                >project page</a
              >
              /
              <a href="https://www.youtube.com/watch?v=4XyDdvhhjVo">video</a> /
              <a href="https://arxiv.org/abs/2012.03927">arXiv</a>
              <p></p>
              <p>
                Using neural approximations of expensive visibility integrals
                lets you recover relightable NeRF-like models.
              </p>
            </div>
          
          </article>

          <article>
              <div class="one">
                <div class="two" id="winr_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/notre_160.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/notre.jpg" width="160" />
              </div>

            <div class="article-info">
              
              <a href="http://www.matthewtancik.com/learnit">
                <papertitle
                  >Learned Initializations for Optimizing Coordinate-Based
                  Neural Representations</papertitle
                >
              </a>
              <div>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://people.eecs.berkeley.edu/~bmild/"
                >Ben Mildenhall*</a
              >,
              <a href="https://www.linkedin.com/in/terrance-wang/"
                >Terrance Wang</a
              >,
              <a href="https://www.linkedin.com/in/divi-schmidt-262044180/"
                >Divi Schmidt</a
              >,  
              <a href="https://people.eecs.berkeley.edu/~pratul/"
                >Pratul Srinivasan</a
              >, <strong>Jonathan T. Barron</strong>,
              <a
                href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html"
                >Ren Ng</a
              >
            </div>
            <div>
              <em>CVPR</em>, 2021 &nbsp;
              <font color="red"><strong>(Oral Presentation)</strong></font> </div>
               
              <a href="http://www.matthewtancik.com/learnit">project page</a> /
              <a href="https://www.youtube.com/watch?v=A-r9itCzcyo">video</a> /
              <a href="https://arxiv.org/abs/2012.02189">arXiv</a>
              <p></p>
              <p>
                Using meta-learning to find weight initializations for
                coordinate-based MLPs allows them to converge faster and
                generalize better.
              </p>
            
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="nerfw_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/nerfw_after.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/nerfw_before.jpg" width="160" />
              </div>
              
              <div class='article-info'>
              <a href="https://nerf-w.github.io/">
                <papertitle
                  >NeRF in the Wild: Neural Radiance Fields for Unconstrained
                  Photo Collections</papertitle
                >
              </a>
               <div>
              <a href="http://www.ricardomartinbrualla.com/"
                >Ricardo Martin-Brualla*</a
              >,
              <a
                href="https://scholar.google.com/citations?user=g98QcZUAAAAJ&hl=en"
                >Noha Radwan*</a
              >,
              <a href="https://research.google/people/105804/"
                >Mehdi S. M. Sajjadi*</a
              >,  
              <strong>Jonathan T. Barron</strong>,
              <a
                href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en"
                >Alexey Dosovitskiy</a
              >,
              <a href="http://www.stronglyconvex.com/about.html"
                >Daniel Duckworth</a
              >
               </div>
              <div><em>CVPR</em>, 2021 &nbsp;
              <font color="red"><strong>(Oral Presentation)</strong></font> </div>
               
              <a href="https://nerf-w.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2008.02268">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA">video</a>
              <p></p>
              <p>
                Letting NeRF reason about occluders and appearance variation
                produces photorealistic view synthesis using only unstructured
                internet photos.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="inerf_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/inerf_after.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/inerf_before.jpg" width="160" />
              </div>
              <div class='article-info'>
              <a href="http://yenchenlin.me/inerf/">
                <papertitle
                  >iNeRF: Inverting Neural Radiance Fields for Pose
                  Estimation</papertitle
                >
              </a>
               <div>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>,
              <a href="http://www.peteflorence.com/">Pete Florence</a>,
              <strong>Jonathan T. Barron</strong>,  
              <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU"
                >Alberto Rodriguez</a
              >, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
              <a
                href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en"
                >Tsung-Yi Lin</a
              >
               </div>
              <div><em>arXiv</em>, 2020</div>
               
              <a href="http://yenchenlin.me/inerf/">project page</a> /
              <a href="https://arxiv.org/abs/2012.05877">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=eQuCZaQN0tI">video</a>
              <p></p>
              <p>
                Given an image of an object and a NeRF of that object, you can
                estimate that object's pose.
              </p>
              </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="nerd_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/nerd_160.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/nerd_160.jpg" width="160" />
              </div>
              
              <div class='article-info'>
              <a href="https://markboss.me/publication/2021-nerd/">
                <papertitle
                  >NeRD: Neural Reflectance Decomposition from Image
                  Collections</papertitle
                >
              </a>
               
              <div>
              <a href="https://markboss.me">Mark Boss</a>,
              <a
                href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/"
                >Raphael Braun</a
              >, <a href="https://varunjampani.github.io">Varun Jampani</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
              <a
                href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/"
                >Hendrik P. A. Lensch</a
              >
              </div>

              <div><em>arXiv</em>, 2020</div>
               
              <a href="https://markboss.me/publication/2021-nerd/"
                >project page</a
              >
              /
              <a href="https://www.youtube.com/watch?v=JL-qMTXw9VU">video</a> /
              <a
                href="https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition"
                >code</a
              >
              /
              <a href="https://arxiv.org/abs/2012.03918">arXiv</a>
              <p></p>
              <p>
                A NeRF-like model that can decompose (and mesh) objects with
                non-Lambertian reflectances, complex geometry, and unknown
                illumination.
              </p>
            
          </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="nerfie_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/nerfie_after.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/nerfie_before.jpg" width="160" />
              </div>
              
              <div class='article-info'>
              <a href="https://nerfies.github.io/">
                <papertitle>Deformable Neural Radiance Fields</papertitle>
              </a>
               
            <div>
              <a href="https://keunhong.com">Keunhong Park</a>,
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>,  
              <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
              <a href="https://homes.cs.washington.edu/~seitz/"
                >Steven M. Seitz</a
              >,
              <a href="http://www.ricardomartinbrualla.com"
                >Ricardo-Martin Brualla</a
              >
              </div> 
              <div><em>arXiv</em>, 2020</div>
               
              <a href="https://nerfies.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2011.12948">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA">video</a>
              <p></p>
              <p>
                Building deformation fields into NeRF lets you capture non-rigid
                subjects, like people.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="flare_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/flare_after.jpg" width="160" />
                </div>
                <img src="images/flare_before.jpg" width="160" />
              </div>
              <div class='article-info'>
              <a href="https://arxiv.org/abs/2011.12485">
                <papertitle>Single-Image Lens Flare Removal</papertitle>
              </a>
               <div>
              <a href="http://yicheng.rice.edu/">Yicheng Wu</a>,
              <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ"
                >Qiurui He</a
              >, <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,  
              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
              <a
                href="https://computationalimaging.rice.edu/team/ashok-veeraraghavan/"
                >Ashok Veeraraghavan</a
              >,
              <strong>Jonathan T. Barron</strong>
               </div>
              <div><em>arXiv</em>, 2020</div>
               
              <a href="https://yichengwu.github.io/flare-removal/"
                >project page</a
              >
              /
              <a href="https://arxiv.org/abs/2011.12485">arXiv</a>
              <p></p>
              <p>
                Simulating the optics of a camera's lens lets you train a model
                that removes lens flare from a single image.
              </p>
            </div>
          </article>

          <article>
              <div class="one">
                <div class="two" id="c5_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/c5_after.jpg" width="160" />
                </div>
                <img src="images/c5_before.jpg" width="160" />
              </div>
            <div class='article-info'>
              <a href="https://arxiv.org/abs/2011.11890">
                <papertitle
                  >Cross-Camera Convolutional Color Constancy</papertitle
                >
              </a>
               
              <div>
                <a href="https://sites.google.com/corp/view/mafifi"
                  >Mahmoud Afifi</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="http://www.chloelegendre.com/">Chloe LeGendre</a>,
                <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                <a href="https://www.linkedin.com/in/fbleibel/"
                  >Francois Bleibel</a
                >
              </div>
               
              <div><em>arXiv</em>, 2020</div>
               
              <p></p>
              <p>
                With some extra (unlabeled) test-set images, you can build a
                hypernetwork that calibrates itself at test time to
                previously-unseen cameras.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="lssr_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/lssr_after.jpg" width="160" />
                </div>
                <img src="images/lssr_before.jpg" width="160" />
              </div>
              
            
              <div class="article-info"> 
              <a
                href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/"
              >
                <papertitle
                  >Light Stage Super-Resolution: Continuous High-Frequency
                  Relighting</papertitle
                >
              </a>
               
              <div>
                <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>
                <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="http://www.seanfanello.it/">Sean Fanello</a>,
                <a
                  href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en"
                  >Christoph Rhemann</a
                >, <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              </div>
               
              <div><em>SIGGRAPH Asia</em>, 2020</div>
               
              <a
                href="http://cseweb.ucsd.edu/~viscomp/projects/SIGA20LightstageSuperres/"
                >project page</a
              >
              /
              <a href="https://arxiv.org/abs/2010.08888">arXiv</a>
              <p></p>
              <p>
                Scans for light stages are inherently aliased, but we can use
                learning to super-resolve them.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="dualrefl_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/dualrefl_after.jpg" width="160" />
                </div>
                <img src="images/dualrefl_before.jpg" width="160" />
              </div>
              
            
              <div class="article-info"> 
              <a href="http://sniklaus.com/dualref">
                <papertitle>Learned Dual-View Reflection Removal</papertitle>
              </a>
               
              <div>
                <a href="http://sniklaus.com/welcome">Simon Niklaus</a>,
                <a href="https://people.eecs.berkeley.edu/~cecilia77/"
                  >Xuaner (Cecilia) Zhang</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                <a href="http://web.cecs.pdx.edu/~fliu/">Feng Liu</a>,
                <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              </div>
                <div><em>WACV</em>, 2021</div>
                 
                <a href="http://sniklaus.com/dualref">project page</a> /
                <a href="https://arxiv.org/abs/2010.00702">arXiv</a>
              
              
              <p>
                Reflections and the things behind them often exhibit parallax,
                and this lets you remove reflections from stereo pairs.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="nlt_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/nlt_after.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/nlt_before.jpg" width="160" />
              </div>
              
            
              <div class="article-info"> 
              <a href="http://nlt.csail.mit.edu/">
                <papertitle
                  >Neural Light Transport for Relighting and View
                  Synthesis</papertitle
                >
              </a>
               
              <div>
                <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="http://www.seanfanello.it/">Sean Fanello</a>,
                <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                <a href="https://research.google/people/106687/">Rohit Pandey</a>,
                <a href="https://www.dtic.ua.es/~sorts/">Sergio Orts-Escolano</a>,
                <a href="https://dl.acm.org/profile/99659224296"
                  >Philip Davidson</a
                >,
                <a
                  href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en"
                  >Christoph Rhemann</a
                >, <a href="http://www.pauldebevec.com/">Paul Debevec</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <a href="http://billf.mit.edu/">William T. Freeman</a>
              </div>
               
              <div><em>ACM TOG</em>, 2021</div>
               
              <a href="http://nlt.csail.mit.edu/">project page</a> /
              <a href="https://arxiv.org/abs/2008.03806">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=OGEnCWZihHE">video</a>
              <p></p>
              <p>
                Embedding a convnet within a predefined texture atlas enables
                simultaneous view synthesis and relighting.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="ff_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/lion_ff.jpg" width="160" />
                </div>
                <img src="images/lion_none.jpg" width="160" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://people.eecs.berkeley.edu/~bmild/fourfeat/index.html"
              >
                <papertitle
                  >Fourier Features Let Networks Learn High Frequency Functions
                  in Low Dimensional Domains</papertitle
                >
              </a>
               
              <div>
                <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
                <a href="https://people.eecs.berkeley.edu/~pratul/"
                  >Pratul Srinivasan*</a
                >,
                <a href="https://people.eecs.berkeley.edu/~bmild/"
                  >Ben Mildenhall*</a
                >,
                <a href="https://people.eecs.berkeley.edu/~sfk/"
                  >Sara Fridovich-Keil</a
                >,
                <a href="https://www.linkedin.com/in/nithinraghavan"
                  >Nithin Raghavan</a
                >,
                <a
                  href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en"
                  >Utkarsh Singhal</a
                >, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <strong>Jonathan T. Barron</strong>,
                <a
                  href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html"
                  >Ren Ng</a
                >
              </div>
               
              <div>
                <em>NeurIPS</em>, 2020 &nbsp
                <font color="#FF8080"><strong>(Spotlight)</strong></font>
              </div>
               
              <a href="https://people.eecs.berkeley.edu/~bmild/fourfeat/"
                >project page</a
              >
              / video:
              <a href="https://www.youtube.com/watch?v=nVA6K6Sn2S4">3 min</a>,
              <a href="https://www.youtube.com/watch?v=iKyIJ_EtSkw">10 min</a> /
              <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
              <a href="https://github.com/tancik/fourier-feature-networks"
                >code</a
              >
              <p></p>
              <p>
                Composing neural networks with a simple Fourier feature mapping
                allows them to learn detailed high-frequency functions.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="thresh_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/thresh_after.png" width="160" />
                </div>
                <img src="images/thresh_before.jpg" width="160" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/2007.07350">
                <papertitle
                  >A Generalization of Otsu's Method and Minimum Error
                  Thresholding</papertitle
                >
              </a>
               
              <div><strong>Jonathan T. Barron</strong></div>
               
              <div><em>ECCV</em>, 2020 &nbsp
              <font color="#FF8080"><strong>(Spotlight)</strong></font></div>
               
              <a href="https://github.com/jonbarron/hist_thresh">code</a> /
              <a href="https://www.youtube.com/watch?v=rHtQQlQo1Q4">video</a> /
              <a href="data/BarronECCV2020.bib">bibtex</a>
               
              <p></p>
              <p>
                A simple and fast Bayesian algorithm that can be written in ~10
                lines of code outperforms or matches giant CNNs on image
                binarization, and unifies three classic thresholding algorithms.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="uflow_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/uflow_after.png" width="160" />
                </div>
                <img src="images/uflow_before.jpg" width="160" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/2006.04902">
                <papertitle
                  >What Matters in Unsupervised Optical Flow</papertitle
                >
              </a>
               
              <div>
                <a href="http://ricojonschkowski.com/">Rico Jonschkowski</a>,
                <a
                  href="https://www.linkedin.com/in/austin-charles-stone-1ba33b138/"
                  >Austin Stone</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="https://research.google/people/ArielGordon/"
                  >Ariel Gordon</a
                >,
                <a href="https://www.linkedin.com/in/kurt-konolige/"
                  >Kurt Konolige</a
                >,
                <a href="https://research.google/people/AneliaAngelova/"
                  >Anelia Angelova</a
                >
              </div>
               
              <div><em>ECCV</em>, 2020 &nbsp
              <font color="red"><strong>(Oral Presentation)</strong></font></div>
               
              <a
                href="https://github.com/google-research/google-research/tree/master/uflow"
                >code</a
              >
               
              <p></p>
              <p>
                Extensive experimentation yields a simple optical flow technique
                that is trained on only unlabeled videos, but still works as
                well as supervised techniques.
              </p>
            </div>
          </article>

          <article class="highlight"
          >
            
              <div class="one">
                <div class="two" id="nerf_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/vase_small.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/vase_still.png" width="160" />
              </div>
              
              <div class="article-info"> 
              <a href="http://www.matthewtancik.com/nerf">
                <papertitle
                  >NeRF: Representing Scenes as Neural Radiance Fields for View
                  Synthesis</papertitle
                >
              </a>
               
              <div>
                <a href="https://people.eecs.berkeley.edu/~bmild/"
                  >Ben Mildenhall*</a
                >,
                <a href="https://people.eecs.berkeley.edu/~pratul/"
                  >Pratul Srinivasan*</a
                >, <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <a
                  href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html"
                  >Ren Ng</a
                >
              </div>
               
              <div><em>ECCV</em>, 2020 &nbsp
              <font color="red"
                ><strong
                  >(Oral Presentation, Best Paper Honorable Mention)</strong
                ></font
              ></div>
               
              <a href="http://www.matthewtancik.com/nerf">project page</a>
              /
              <a href="https://arxiv.org/abs/2003.08934">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo&t"
                >talk video</a
              >
              /
              <a href="https://www.youtube.com/watch?v=JuH79E8rdKc"
                >supp video</a
              >
              /
              <a href="https://github.com/bmild/nerf">code</a>
              <p></p>
              <p>
                Training a tiny non-convolutional neural network to reproduce a
                scene using volume rendering achieves photorealistic view
                synthesis.
              </p>
            </div>
          </article>

          <article
          >
            
              <div class="one">
                <div class="two" id="porshadmanip_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/porshadmanip_after.jpg" width="160" />
                </div>
                <img src="images/porshadmanip_before.jpg" width="160" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/2005.08925">
                <papertitle>Portrait Shadow Manipulation</papertitle>
              </a>
               
              <div>
                <a href="https://people.eecs.berkeley.edu/~cecilia77/"
                  >Xuaner (Cecilia) Zhang</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a
                >,
                <a href="https://www.linkedin.com/in/rohit-pandey-bab10b7a/"
                  >Rohit Pandey</a
                >,
                <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
                <a href="http://graphics.stanford.edu/~dejacobs/"
                  >David E. Jacobs</a
                >
              </div>
               
              <div><em>SIGGRAPH</em>, 2020</div>
               
              <a
                href="https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait"
                >project page</a
              >
              /
              <a href="https://www.youtube.com/watch?v=M_qYTXhzyac">video</a>
              <p></p>
              <p>
                Networks can be trained to remove shadows cast on human faces
                and to soften harsh lighting.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="learnaf_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/learnaf_after.jpg" width="160" />
                </div>
                <img src="images/learnaf_before.jpg" width="160" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/2004.12260">
                <papertitle>Learning to Autofocus</papertitle>
              </a>
               
              <div>
                <a href="">Charles Herrmann</a>,
                <a href="">Richard Strong Bowen</a>,
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ"
                  >Qiurui He</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.cornell.edu/~rdz/index.htm">Ramin Zabih</a>
              </div>
               
              <div><em>CVPR</em>, 2020</div>
               
              <a href="https://arxiv.org/abs/2004.12260">arXiv</a>
              <p></p>
              <p>
                Machine learning can be used to train cameras to autofocus
                (which is not the same problem as "depth from defocus").
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="lh_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <video width="100%" height="100%" muted autoplay loop>
                    <source src="images/rings_crop.mp4" type="video/mp4" />
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="images/rings.png" width="160" />
              </div>
              
              <div class="article-info"> 
              <a href="https://people.eecs.berkeley.edu/~pratul/lighthouse/">
                <papertitle
                  >Lighthouse: Predicting Lighting Volumes for
                  Spatially-Coherent Illumination</papertitle
                >
              </a>
               
              <div>
                <a href="https://people.eecs.berkeley.edu/~pratul/"
                  >Pratul Srinivasan*</a
                >,
                <a href="https://people.eecs.berkeley.edu/~bmild/"
                  >Ben Mildenhall*</a
                >, <a href="http://matthewtancik.com/">Matthew Tancik</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://research.google/people/RichardTucker/"
                  >Richard Tucker</a
                >,
                <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
                 
              </div>
              <div><em>CVPR</em>, 2020</div>
               
              <a href="https://people.eecs.berkeley.edu/~pratul/lighthouse/"
                >project page</a
              >
              /
              <a href="https://github.com/pratulsrinivasan/lighthouse">code</a>
              /
              <a href="https://arxiv.org/abs/2003.08367">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=KsiZpUFPqIU">video</a>
              <p></p>
              <p>
                We predict a volume from an input stereo pair that can be used
                to calculate incident lighting at any 3D point within a scene.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="skyopt_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/skyopt_after.jpg" width="160" />
                </div>
                <img src="images/skyopt_before.jpg" width="160" />
              </div>
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/2006.10172">
                <papertitle
                  >Sky Optimization: Semantically Aware Image Processing of
                  Skies in Low-Light Photography</papertitle
                >
              </a>
               
              <div>
                <a href="https://sites.google.com/corp/view/orly-liba/"
                  >Orly Liba</a
                >,
                <a href="https://www.linkedin.com/in/longqicai/en-us"
                  >Longqi Cai</a
                >,
                <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a
                >,
                <a href="https://research.google/people/EladEban/">Elad Eban</a>,
                <a href="https://research.google/people/YairMovshovitzAttias/"
                  >Yair Movshovitz-Attias</a
                >,
                <a href="https://scholar.google.com/citations?user=2jXxOYQAAAAJ"
                  >Yael Pritch</a
                >,
                <a href="https://www.linkedin.com/in/huizhong-chen-00776432"
                  >Huizhong Chen</a
                >,
                <strong>Jonathan T. Barron</strong>
              </div>
               
              <div><em>NTIRE CVPRW</em>, 2020</div>
               
              <a href="https://google.github.io/sky-optimization/"
                >project page</a
              >
              <p></p>
              <p>
                If you want to photograph the sky, it helps to know where the
                sky is.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="nightsight_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/nightsight_after.jpg" />
                </div>
                <img src="images/nightsight_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/1910.11336">
                <papertitle
                  >Handheld Mobile Photography in Very Low Light</papertitle
                >
              </a>
               
              <div>
                <a href="https://sites.google.com/site/orlylibaprofessional/"
                  >Orly Liba</a
                >,
                <a href="https://scholar.google.com/citations?user=6PhlPWMAAAAJ"
                  >Kiran Murthy</a
                >,
                <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a
                >, <a href="https://www.timothybrooks.com/">Timothy Brooks</a>,
                <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                <a href="https://scholar.google.com/citations?user=qgc_jY0AAAAJ"
                  >Nikhil Karnad</a
                >,
                <a href="https://scholar.google.com/citations?user=BxqV_RsAAAAJ"
                  >Qiurui He</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="https://ai.google/research/people/105641/"
                  >Dillon Sharlet</a
                >, <a href="http://www.geisswerks.com/">Ryan Geiss</a>,
                <a href="https://people.csail.mit.edu/hasinoff/"
                  >Samuel W. Hasinoff</a
                >,
                <a href="https://scholar.google.com/citations?user=2jXxOYQAAAAJ"
                  >Yael Pritch</a
                >,
                <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
              </div>
               
              <div><em>SIGGRAPH Asia</em>, 2019</div>
               
              <a href="https://github.com/google/night-sight/tree/master/docs"
                >project page</a
              >
               
              <p></p>
              <p>
                By rethinking metering, white balance, and tone mapping, we can
                take pictures in places too dark for humans to see clearly.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="font_image" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/font_after.png" />
                </div>
                <img src="images/font_before.png" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/1910.00748">
                <papertitle
                  >A Deep Factorization of Style and Structure in
                  Fonts</papertitle
                >
              </a>
               
              <div>
                <a href="http://www.cs.cmu.edu/~asrivats/">Nikita Srivatsan</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://people.eecs.berkeley.edu/~klein/">Dan Klein</a>,
                <a href="http://cseweb.ucsd.edu/~tberg/"
                  >Taylor Berg-Kirkpatrick</a
                >
              </div>
               
              <div><em>EMNLP</em>, 2019 &nbsp
              <font color="red"><strong>(Oral Presentation)</strong></font></div>
               
              <p></p>
              <p>
                Variational auto-encoders can be used to disentangle a
                characters style from its content.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="dpzlearn_image"onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/dpzlearn_after.jpg" />
                </div>
                <img src="images/dpzlearn_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/1904.05822">
                <papertitle
                  >Learning Single Camera Depth Estimation using
                  Dual-Pixels</papertitle
                >
              </a>
               
              <div>
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="">Sameer Ansari,</a>,
                <strong>Jonathan T. Barron</strong>
              </div>
               
              <div><em>ICCV</em>, 2019 &nbsp
              <font color="red"><strong>(Oral Presentation)</strong></font></div>
               
              <a
                href="https://github.com/google-research/google-research/tree/master/dual_pixels"
                >code</a
              >
              /
              <a href="data/GargICCV2019.bib">bibtex</a>
              <p></p>
              <p>
                Considering the optics of dual-pixel image sensors improves
                monocular depth estimation techniques.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="porlight_image"onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/porlight_after.jpg" />
                </div>
                <img src="images/porlight_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="http://cseweb.ucsd.edu/~viscomp/projects/SIG19PortraitRelighting/"
              >
                <papertitle>Single Image Portrait Relighting</papertitle>
              </a>
               
              <div>
                <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a
                >, <a href="https://cseweb.ucsd.edu/~zex014/">Zexiang Xu</a>,
                Xueming Yu,
                <a href="http://ict.usc.edu/profile/graham-fyffe/">Graham Fyffe</a
                >, Christoph Rhemann, Jay Busch,
                <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>
              </div>
               
              <div><em>SIGGRAPH</em>, 2019</div>
               
              <a
                href="http://cseweb.ucsd.edu/~viscomp/projects/SIG19PortraitRelighting/"
                >project page</a
              >
              / <a href="https://arxiv.org/abs/1905.00824">arxiv</a> /
              <a href="https://www.youtube.com/watch?v=yxhGWds_g4I">video</a> /
              <a
                href="https://petapixel.com/2019/07/16/researchers-developed-an-ai-that-can-relight-portraits-after-the-fact/"
                >press</a
              >
              /
              <a href="data/SunSIGGRAPH2019.bib">bibtex</a>
               
              <p></p>
              <p>
                Training a neural network on light stage scans and environment
                maps produces an effective relighting method.
              </p>
            </div>
          </article>

          <article class="highlight"
          >
            
              <div class="one">
                <div class="two" id="loss_image"onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/loss_after.png" />
                </div>
                <img src="images/loss_before.png" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq"
              >
                <papertitle
                  >A General and Adaptive Robust Loss Function</papertitle
                >
              </a>
               
              <div><strong>Jonathan T. Barron</strong></div>
               
              <div><em>CVPR</em>, 2019 &nbsp
              <font color="red"
                ><strong
                  >(Oral Presentation, Best Paper Award Finalist)</strong
                ></font
              ></div>
               
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a
                href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4"
                >supplement</a
              >
              / <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s"
                >talk</a
              >
              /
              <a
                href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing"
                >slides</a
              >
              / code:
              <a
                href="https://github.com/google-research/google-research/tree/master/robust_loss"
                >TF</a
              >,
              <a
                href="https://github.com/google-research/google-research/tree/master/robust_loss_jax"
                >JAX</a
              >,
              <a href="https://github.com/jonbarron/robust_loss_pytorch"
                >pytorch</a
              >
              / <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>
                A single robust loss function is a superset of many other common
                robust loss functions, and allows training to automatically
                adapt the robustness of its own loss.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" id="mpi_image"onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/mpi_after.jpg" />
                </div>
                <img src="images/mpi_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1TU5L6fnt4Kd49IUOU7aNxor5NIgdHuNG/view?usp=sharing"
              >
                <papertitle
                  >Pushing the Boundaries of View Extrapolation with Multiplane
                  Images</papertitle
                >
              </a>
               
              <div>
                <a href="https://people.eecs.berkeley.edu/~pratul/"
                  >Pratul P. Srinivasan</a
                >, Richard Tucker, <strong>Jonathan T. Barron</strong>,
                <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
                <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>
              </div>
               
              <div><em>CVPR</em>, 2019 &nbsp
              <font color="red"
                ><strong
                  >(Oral Presentation, Best Paper Award Finalist)</strong
                ></font
              ></div>
               
              <a
                href="https://drive.google.com/file/d/1GUW_n-BAn9Q4VntEA_OTHNJiHO7XfC62/view?usp=sharing"
                >supplement</a
              >
              /
              <a href="https://www.youtube.com/watch?v=aJqAaMNL2m4">video</a> /
              <a href="data/SrinivasanCVPR2019.bib">bibtex</a>
              <p></p>
              <p>
                View extrapolation with multiplane images works better if you
                reason about disocclusions and disparity sampling frequencies.
              </p>
            
          </article>

          <article
          >
            
              <div class="one">
                <div class="two" id="unprocessing_image"onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)">
                  <img src="images/unprocessing_after.jpg" />
                </div>
                <img src="images/unprocessing_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1H0Wtd--un2JN76dUJN8iC9fWfkA16n8D/view?usp=sharing"
              >
                <papertitle
                  >Unprocessing Images for Learned Raw Denoising</papertitle
                >
              </a>
               
              <div>
                <a href="http://timothybrooks.com/">Tim Brooks</a>,
                <a href="https://people.eecs.berkeley.edu/~bmild/"
                  >Ben Mildenhall</a
                >, <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
                <strong>Jonathan T. Barron</strong>
              </div>
               
              <div><em>CVPR</em>, 2019 &nbsp
              <font color="red"><strong>(Oral Presentation)</strong></font></div>
               
              <a href="https://arxiv.org/abs/1811.11127">arxiv</a> /
              <a href="http://timothybrooks.com/tech/unprocessing/"
                >project page</a
              >
              /
              <a
                href="https://github.com/google-research/google-research/tree/master/unprocessing"
                >code</a
              >
              /
              <a href="data/BrooksCVPR2019.bib">bibtex</a>
              <p></p>
              <p>
                We can learn a better denoising model by processing and
                unprocessing images the same way a camera does.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="motionblur_image">
                  <img src="images/motionblur_after.jpg" />
                </div>
                <img src="images/motionblur_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1hWpA4f6iLVcOkZI3zEAAWKARSQhnVgbY/view?usp=sharing"
              >
                <papertitle>Learning to Synthesize Motion Blur</papertitle>
              </a>
               
              <div>
                <a href="http://timothybrooks.com/">Tim Brooks</a>,
                <strong>Jonathan T. Barron</strong>
              </div>
               
              <div><em>CVPR</em>, 2019 &nbsp
              <font color="red"><strong>(Oral Presentation)</strong></font></div>
               
              <a href="https://arxiv.org/abs/1811.11745">arxiv</a> /
              <a
                href="https://drive.google.com/file/d/1dUQwBMmQdYYIP0zHR_nDQY-uQbaMdcSN/view?usp=sharing"
                >supplement</a
              >
              /
              <a href="http://timothybrooks.com/tech/motion-blur/"
                >project page</a
              >
              /
              <a href="https://www.youtube.com/watch?v=8T1jjSz-2V8">video</a> /
              <a
                href="https://github.com/google-research/google-research/tree/master/motion_blur"
                >code</a
              >
              /
              <a href="data/BrooksBarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>
                Frame interpolation techniques can be used to train a network
                that directly synthesizes linear blur kernels.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="darkflash_image">
                  <img src="images/darkflash_after.png" />
                </div>
                <img src="images/darkflash_before.png" />
              </div>
              
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/1901.01370">
                <papertitle
                  >Stereoscopic Dark Flash for Low-light Photography</papertitle
                >
              </a>
               
              <div>
                <a href="https://www.andrew.cmu.edu/user/jianwan2/">Jian Wang</a>,
                <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>
              </div>
               
              <div><em>ICCP</em>, 2019</div>
               
              <p></p>
              <p>
                By making one camera in a stereo pair hyperspectral we can
                multiplex dark flash pairs in space instead of time.
              </p>
            </div>
          </article>

          <article
          >
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="motionstereo_image">
                  <img src="images/motionstereo_after.png" />
                </div>
                <img src="images/motionstereo_before.png" />
              </div>
              
            

              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1AABFJ3NgD5DAo5JEpEjWZrcQNzjZnvW9/view?usp=sharing"
              >
                <papertitle>Depth from Motion for Smartphone AR</papertitle>
              </a>
               
              <div>
                <a href="https://www.linkedin.com/in/valentinjulien/"
                  >Julien Valentin</a
                >,
                <a href="https://www.linkedin.com/in/adarshkowdle/"
                  >Adarsh Kowdle</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>, and others
              </div>
                <div><em>SIGGRAPH Asia</em>, 2018</div>
                 
                <a href="https://github.com/jonbarron/planar_filter"
                  >planar filter toy code</a
                >
                /
                <a href="data/Valentin2018.bib">bibtex</a>
              
              
              <p>
                Depth cues from camera motion allow for real-time occlusion
                effects in augmented reality applications.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="portrait_image">
                  <img src="images/portrait_after.jpg" />
                </div>
                <img src="images/portrait_before.jpg" />
              </div>
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/13i6DlS9UhGVKmwslLUFnKBwdxFRVQeQj/view?usp=sharing"
              >
                <papertitle
                  >Synthetic Depth-of-Field with a Single-Camera Mobile
                  Phone</papertitle
                >
              </a>
               
              <div>
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                <a href="http://graphics.stanford.edu/~dejacobs/"
                  >David E. Jacobs</a
                >, Bryan E. Feldman, Nori Kanazawa, Robert Carroll,
                <a href="http://www.cs.cmu.edu/~ymovshov/"
                  >Yair Movshovitz-Attias</a
                >, <strong>Jonathan T. Barron</strong>, Yael Pritch,
                <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
                 
              </div>
              <div><em>SIGGRAPH</em>, 2018</div>
               
              <a href="https://arxiv.org/abs/1806.04171">arxiv</a> /
              <a
                href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html"
                >blog post</a
              >
              /
              <a href="data/Wadhwa2018.bib">bibtex</a>
              <p></p>
              <p>
                Dual pixel cameras and semantic segmentation algorithms can be
                used for shallow depth of field effects.
              </p>
              <p>
                This system is the basis for "Portrait Mode" on the Google Pixel
                2 smartphones
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="aperture_image">
                  <img src="images/aperture_after.jpg" />
                </div>
                <img src="images/aperture_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1MpvxcW7OTJP321QL_q4ZLQ8D653bZZzy/view?usp=sharing"
              >
                <papertitle
                  >Aperture Supervision for Monocular Depth
                  Estimation</papertitle
                >
              </a>
               
              <div>
                <a href="https://people.eecs.berkeley.edu/~pratul/"
                  >Pratul P. Srinivasan</a
                >, <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,
                <strong>Jonathan T. Barron</strong>
              </div>
               
              <div><em>CVPR</em>, 2018</div>
               
              <a href="https://github.com/google/aperture_supervision">code</a>
              /
              <a href="data/Srinivasan2018.bib">bibtex</a>
              <p></p>
              <p>
                Varying a camera's aperture provides a supervisory signal that
                can teach a neural network to do monocular depth estimation.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="deepburst_image">
                  <img src="images/deepburst_after.png" />
                </div>
                <img src="images/deepburst_before.png" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1GAH8ijyZ7GnoBnQFANEzdXinHrE4vvXn/view?usp=sharing"
              >
                <papertitle
                  >Burst Denoising with Kernel Prediction Networks</papertitle
                >
              </a>
               
              <div>
                <a href="https://people.eecs.berkeley.edu/~bmild/"
                  >Ben Mildenhall</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
                <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>, Robert
                Carroll
              </div>
               
              <div><em>CVPR</em>, 2018 &nbsp
              <font color="#FF8080"><strong>(Spotlight)</strong></font></div>
               
              <a
                href="https://drive.google.com/file/d/1aqk3Q-L2spjLZh2yRWKUWIDcZkGjQ7US/view?usp=sharing"
                >supplement</a
              >
              / <a href="https://github.com/google/burst-denoising">code</a> /
              <a href="data/Mildenhall2018.bib">bibtex</a>
              <p></p>
              <p>
                We train a network to predict linear kernels that denoise noisy
                bursts from cellphone cameras.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="friendly_image">
                  <img src="images/friendly_after.png" />
                </div>
                <img src="images/friendly_before.png" />
              </div>
             
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1w_0djhL0QgC_fbehnJ0c-J23_kW_420p/view?usp=sharing"
              >
                <papertitle
                  >A Hardware-Friendly Bilateral Solver for Real-Time Virtual
                  Reality Video</papertitle
                >
              </a>
               
              <div>
                <a href="https://homes.cs.washington.edu/~amrita/"
                  >Amrita Mazumdar</a
                >,
                <a href="http://homes.cs.washington.edu/~armin/">Armin Alaghi</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>,
                <a href="https://homes.cs.washington.edu/~luisceze/">Luis Ceze</a
                >,
                <a href="https://homes.cs.washington.edu/~oskin/">Mark Oskin</a>,
                <a href="http://homes.cs.washington.edu/~seitz/"
                  >Steven M. Seitz</a
                >
              </div>
               
              <div><em>High-Performance Graphics (HPG)</em>, 2017</div>
               
              <a href="https://sampa.cs.washington.edu/projects/vr-hw.html"
                >project page</a
              >
              <p></p>
              <p>
                A reformulation of the bilateral solver can be implemented
                efficiently on GPUs and FPGAs.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="hdrnet_image">
                  <img src="images/hdrnet_after.jpg" />
                </div>
                <img src="images/hdrnet_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1jQY3CTMnLX7PeGUzYLso9H1eCsZyWbwg/view?usp=sharing"
              >
                <papertitle
                  >Deep Bilateral Learning for Real-Time Image
                  Enhancement</papertitle
                >
              </a>
               
              <div>
                <a href="http://www.mgharbi.com">Micha&euml;l Gharbi</a>,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://people.csail.mit.edu/hasinoff/"
                  >Samuel W. Hasinoff</a
                >,
                <a href="http://people.csail.mit.edu/fredo/"
                  >Fr&eacute;do Durand
                </a>
              </div>
               
              <div><em>SIGGRAPH</em>, 2017</div>
               
              <a href="https://groups.csail.mit.edu/graphics/hdrnet/"
                >project page</a
              >
              /
              <a href="https://www.youtube.com/watch?v=GAe0qKKQY_I">video</a> /
              <a href="data/GharbiSIGGRAPH2017.bib">bibtex</a> /
              <a
                href="http://news.mit.edu/2017/automatic-image-retouching-phone-0802"
                >p</a
              ><a
                href="https://www.wired.com/story/googles-new-algorithm-perfects-photos-before-you-even-take-them/"
                >r</a
              ><a
                href="https://petapixel.com/2017/08/02/new-ai-can-retouch-photos-snap/"
                >e</a
              ><a
                href="https://www.theverge.com/2017/8/2/16082272/google-mit-retouch-photos-machine-learning"
                >s</a
              ><a
                href="http://gizmodo.com/clever-camera-app-uses-deep-learning-to-perfectly-retou-1797474282"
                >s</a
              >
              <p></p>
              <p>
                By training a deep network in bilateral space we can learn a
                model for high-resolution and real-time image enhancement.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="ffcc_image">
                  <img src="images/ffcc_after.jpg" />
                </div>
                <img src="images/ffcc_before.jpg" />
              </div>
              <div class="article-info"> 
              <a href="https://arxiv.org/abs/1611.07596">
                <papertitle>Fast Fourier Color Constancy</papertitle>
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://ai.google/research/people/105312/">Yun-Ta Tsai</a
                >
              </div>
               
              <div><em>CVPR</em>, 2017</div>
               
              <a href="https://youtu.be/rZCXSfl13rY">video</a> /
              <a href="data/BarronTsaiCVPR2017.bib">bibtex</a> /
              <a href="https://github.com/google/ffcc">code</a> /
              <a
                href="https://drive.google.com/open?id=0B4nuwEMaEsnmWkJQMlFPSFNzbEk"
                >output</a
              >
              /
              <a
                href="https://blog.google/products/photos/six-tips-make-your-photos-pop/"
                >blog post</a
              >
              /
              <a
                href="https://9to5google.com/2017/03/03/google-photos-auto-white-balance/"
                >p</a
              ><a
                href="https://www.engadget.com/2017/03/03/google-photos-automatically-fixes-your-pictures-white-balance/"
                >r</a
              ><a
                href="https://lifehacker.com/google-photos-will-now-automatically-adjust-the-white-b-1793009155"
                >e</a
              ><a
                href="https://petapixel.com/2017/03/06/google-photos-will-now-automatically-white-balance-snapshots/"
                >s</a
              ><a
                href="http://www.theverge.com/2017/3/3/14800062/google-photos-auto-white-balance-android"
                >s</a
              >
              <p></p>
              <p>
                Color space can be aliased, allowing white balance models to be
                learned and evaluated in the frequency domain. This improves
                accuracy by 13-20% and speed by 250-3000x.
              </p>
              <p>
                This technology is used by
                <a href="https://store.google.com/product/pixel_compare"
                  >Google Pixel</a
                >, <a href="https://photos.google.com/">Google Photos</a>, and
                <a href="https://www.google.com/maps">Google Maps</a>.
              </p>
            </div>
          </article>

          <article class='highlight'
          >
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="jump_image">
                  <img src="images/jump_anim.gif" />
                </div>
                <img src="images/jump_still.png" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1RBnTrtzqmuO8uj3GQaR5vBJZjIC3Jxjn/view?usp=sharing"
              >
                <papertitle>Jump: Virtual Reality Video</papertitle>
              </a>
               
              <div>
                <a href="http://mi.eng.cam.ac.uk/~ra312/">Robert Anderson</a>,
                <a href="https://www.cs.unc.edu/~gallup/">David Gallup</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://mediatech.aalto.fi/~janne/index.php"
                  >Janne Kontkanen</a
                >,
                <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>,
                <a href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>,
                <a href="https://homes.cs.washington.edu/~sagarwal/"
                  >Sameer Agarwal</a
                >,
                <a href="https://homes.cs.washington.edu/~seitz/"
                  >Steven M Seitz</a
                >
              </div>
               
              <div><em>SIGGRAPH Asia</em>, 2016</div>
               
              <a
                href="https://drive.google.com/file/d/11D4eCDXqqFTtZT0WS2COJE0hsAN3QEww/view?usp=sharing"
                >supplement</a
              >
              /
              <a href="https://www.youtube.com/watch?v=O0qUYynupTI">video</a> /
              <a href="data/Anderson2016.bib">bibtex</a> /
              <a
                href="https://blog.google/products/google-vr/jump-using-omnidirectional-stereo-vr-video/"
                >blog post</a
              >
              <p></p>
              <p>
                Using computer vision and a ring of cameras, we can make video
                for virtual reality headsets that is both stereo and 360&deg;.
              </p>
              <p>
                This technology is used by
                <a href="https://vr.google.com/jump/">Jump</a>.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="hdrp_image">
                  <img src="images/hdrp_after.jpg" />
                </div>
                <img src="images/hdrp_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1SSSmVHWbMQ7sZMOredSVWVJXbXobkyzA/view?usp=sharing"
              >
                <papertitle
                  >Burst Photography for High Dynamic Range and Low-Light
                  Imaging on Mobile Cameras</papertitle
                >
              </a>
               
              <div>
                <a href="http://people.csail.mit.edu/hasinoff/"
                  >Samuel W. Hasinoff</a
                >, <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,
                <a href="http://www.geisswerks.com/">Ryan Geiss</a>,
                <a href="http://people.csail.mit.edu/abadams/">Andrew Adams</a>,
                <strong>Jonathan T. Barron</strong>, Florian Kainz,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>
              </div>
               
              <div><em>SIGGRAPH Asia</em>, 2016</div>
               
              <a href="http://hdrplusdata.org/">project page</a> /
              <a
                href="https://drive.google.com/open?id=15EUuSDi1BtHUgQCaiooVrD44qYKIC3vx"
                >supplement</a
              >
              /
              <a href="data/Hasinoff2016.bib">bibtex</a>
              <p></p>
              <p>
                Mobile phones can take beautiful photographs in low-light or
                high dynamic range environments by aligning and merging a burst
                of images.
              </p>
              <p>
                This technology is used by the
                <a
                  href="https://research.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html"
                  >Nexus HDR+</a
                >
                feature.
              </p>
            </div>
          </article>

          <article class='highlight'>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="bs_image">
                  <img src="images/BS_after.jpg" />
                </div>
                <img src="images/BS_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1zFzCaFwkGK1EGmJ_KEqb-ZsRJhfUKN2S/view?usp=sharing"
              >
                <papertitle>The Fast Bilateral Solver</papertitle>
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
              </div>
               
              <div><em>ECCV</em>, 2016 &nbsp
              <font color="red"
                ><strong
                  >(Oral Presentation, Best Paper Honorable Mention)</strong
                ></font
              ></div>
               
              <a href="http://arxiv.org/abs/1511.03296">arXiv</a> /
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmdEREcjhlSXM2NGs/view?usp=sharing"
                >supplement</a
              >
              / <a href="data/BarronPooleECCV2016.bib">bibtex</a> /
              <a
                href="http://videolectures.net/eccv2016_barron_bilateral_solver/"
                >video (they messed up my slides, use &rarr;)</a
              >
              /
              <a
                href="https://drive.google.com/file/d/19x1AeN0PFus6Pjrd8nR-vCmJ6bNEefsC/view?usp=sharing"
                >keynote</a
              >
              (or
              <a
                href="https://drive.google.com/file/d/1p9nduiymK9jUh7WfwlsMjBfW8RoNe_61/view?usp=sharing"
                >PDF</a
              >) /
              <a href="https://github.com/poolio/bilateral_solver">code</a> /
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmaDI3bm5VeDRxams/view?usp=sharing"
                >depth super-res results</a
              >
              /
              <a href="data/BarronPooleECCV2016_reviews.txt">reviews</a>
              <p></p>
              <p>
                Our solver smooths things better than other filters and faster
                than other optimization algorithms, and you can backprop through
                it.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="diverdi_image">
                  <img src="images/diverdi_after.jpg" />
                </div>
                <img src="images/diverdi_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1mmT-LuK_eBZsl3qp4-fAshEPdgfbgvNE/view?usp=sharing"
              >
                <papertitle
                  >Geometric Calibration for Mobile, Stereo, Autofocus
                  Cameras</papertitle
                >
              </a>
               
              <div>
                <a href="http://www.stephendiverdi.com/">Stephen DiVerdi</a>,
                <strong>Jonathan T. Barron</strong>
              </div>
               
              <div><em>WACV</em>, 2016</div>
               
              <a href="data/Diverdi2016.bib">bibtex</a>
              <p></p>
              <p>
                Standard techniques for stereo calibration don't work for cheap
                mobile cameras.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="dt_image">
                  <img src="images/DT_edge.jpg" />
                </div>
                <img src="images/DT_image.jpg" />
              </div>
              
              <div class="article-info"> 
                <a
              href="https://drive.google.com/file/d/178Xj2PZ1w6hZJpucU-TiZOoCemJmvsVQ/view?usp=sharing"
            >
              <papertitle
                >Semantic Image Segmentation with Task-Specific Edge Detection
                Using CNNs and a Discriminatively Trained Domain
                Transform</papertitle
              >
            </a>
              <div>
                <a href="http://liangchiehchen.com/">Liang-Chieh Chen</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://ttic.uchicago.edu/~gpapan/">George Papandreou</a>,
                <a href="http://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>,
                <a href="http://www.stat.ucla.edu/~yuille/">Alan L. Yuille</a>
              </div>
               
              <div><em>CVPR</em>, 2016</div>
              <a href="data/Chen2016.bib">bibtex</a> /
              <a href="http://liangchiehchen.com/projects/DeepLab.html"
                >project page</a
              >
              /
              <a href="https://bitbucket.org/aquariusjay/deeplab-public-ver2"
                >code</a
              >
              <p>
                By integrating an edge-aware filter into a convolutional neural
                network we can learn an edge-detector while improving semantic
                segmentation.
              </p>
            </div>
          </article>

          <article class='highlight'
          >
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="ccc_image">
                  <img src="images/ccc_after.jpg" />
                </div>
                <img src="images/ccc_before.jpg" />
              </div>
              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1id74VNDL8ACrrWf6vYgN2M4kS8gd4n7w/view?usp=sharing"
              >
                <papertitle>Convolutional Color Constancy</papertitle>
              </a>
               
              <div><strong>Jonathan T. Barron</strong></div>
               
              <div><em>ICCV</em>, 2015</div>
               
              <a
                href="https://drive.google.com/file/d/1vO3sVOMihmpNqsuASeR46Y_iME0lOANR/view?usp=sharing"
                >supplement</a
              >
              / <a href="data/BarronICCV2015.bib">bibtex</a> /
              <a href="https://youtu.be/saHwKY9rfx0">video</a> (or
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmalBNUzlENUJSVDg/view?usp=sharing"
                >mp4</a
              >)
              <p></p>
              <p>
                By framing white balance as a chroma localization task we can
                discriminatively learn a color constancy model that beats the
                state-of-the-art by 40%.
              </p>
            </div>
          </article>

          <article>
            
              <div class='one'><img src="images/Shelhamer2015.jpg" /></div>
            
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1stygV71uBruD7Ck9CaAQr7nREvr3DtUL/view?usp=sharing"
              >
                <papertitle
                  >Scene Intrinsics and Depth from a Single Image</papertitle
                >
              </a>
               
              <div>
                <a href="http://imaginarynumber.net/">Evan Shelhamer</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.eecs.berkeley.edu/%7Etrevor/"
                  >Trevor Darrell</a
                >
              </div>
               
              <div><em>ICCV Workshop</em>, 2015</div>
               
              <a href="data/Shelhamer2015.bib">bibtex</a>
              <p></p>
              <p>
                The monocular depth estimates produced by fully convolutional
                networks can be used to inform intrinsic image estimation.
              </p>
            </div>
          </article>

          <article
            class='highlight'
          >
            <div class="one">
              <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="lens_blurry">
                <img src="images/BarronCVPR2015_anim.gif" />
              </div>
              <img src="images/BarronCVPR2015_still.jpg" />
            </div>  

              <div class="article-info"> 
                <a
                href="https://drive.google.com/file/d/1R4RdaBZIs-uJobhIFs9yKf3jIsaHQNH0/view?usp=sharing"
              >
            
              <papertitle
                >Fast Bilateral-Space Stereo for Synthetic Defocus</papertitle
              >
            </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://people.csail.mit.edu/abadams/">Andrew Adams</a>,
                <a href="http://people.csail.mit.edu/yichangshih/">YiChang Shih</a
                >,
                <a href="http://carlos-hernandez.org/">Carlos Hern&aacutendez</a>
              </div>
               
              <div><em>CVPR</em>, 2015 &nbsp
              <font color="red"><strong>(Oral Presentation)</strong></font></div>
               
              <a
                href="https://drive.google.com/file/d/125qgMdqeT1vojMIijIKcOF099LjUgUOL/view?usp=sharing"
                >abstract</a
              >
              /
              <a
                href="https://drive.google.com/file/d/1HGGvVOGxmPjvgdK5q3UD1Qb5Nttg6kq9/view?usp=sharing"
                >supplement</a
              >
              / <a href="data/BarronCVPR2015.bib">bibtex</a> /
              <a
                href="http://techtalks.tv/talks/fast-bilateral-space-stereo-for-synthetic-defocus/61624/"
                >talk</a
              >
              /
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmSzZZdUJSMllSUkE/view?usp=sharing"
                >keynote</a
              >
              (or
              <a
                href="https://drive.google.com/open?id=0B4nuwEMaEsnmZ1ZXUzBCWDJYeFU"
                >PDF</a
              >)
              <p></p>
              <p>
                By embedding a stereo optimization problem in "bilateral-space"
                we can very quickly solve for an edge-aware depth map, letting
                us render beautiful depth-of-field effects.
              </p>
              <p>
                This technology is used by the
                <a
                  href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html"
                  >Google Camera "Lens Blur"</a
                >
                feature.
              </p>
            </div>
          </article>

          <article>
            
              <div class="one">
                <img
                  src="images/PABMM2015.jpg"
                  alt="PontTuset"
                  width="160"
                  style="border-style: none"
                />
              </div>
              
              <div class="article-info"> 
              
                <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal">
                  <papertitle
                    >Multiscale Combinatorial Grouping for Image Segmentation and
                    Object Proposal Generation</papertitle
                  >
                </a>
              <div>
                <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset"
                  >Jordi Pont-Tuset</a
                >,
                <a href="http://www.cs.berkeley.edu/~arbelaez/"
                  >Pablo Arbel&aacuteez</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a
                >,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>TPAMI</em>, 2017</div>
               
              <a
                href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/"
                >project page</a
              >
              / <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a
                href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing"
                >fast eigenvector code</a
              >
              
              <p>
                We produce state-of-the-art contours, regions and object
                candidates, and we compute normalized-cuts eigenvectors 20&times
                faster.
              </p>
              <p>This paper subsumes our CVPR 2014 paper.</p>
            </div>
          </tr>
          </article>
          <article class='highlight'
          >
            
              <div class="one">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="sirfs_image">
                  <a href="images/Estee.png"
                    ><img src="images/Estee_160.png" style="border-style: none"
                  /></a>
                </div>
                <a href="images/Estee.png"
                  ><img
                    src="images/Estee_160_prodB2.png"
                    style="border-style: none"
                /></a>
              </div>
              
              <div class="article-info"> 
                <a href="https://arxiv.org/abs/2010.03592" id="SIRFS">
                  <papertitle
                    >Shape, Illumination, and Reflectance from
                    Shading</papertitle
                  >
                </a>
              
                
                 
                <div>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                </div>
                 
                <div><em>TPAMI</em>, 2015</div>
                 
                <a href="data/BarronMalikTPAMI2015.bib">bibtex</a> /
                <a
                  href="https://drive.google.com/file/d/0B4nuwEMaEsnmVWpfa19mbUxIYW8/view?usp=sharing"
                  >keynote</a
                >
                (or
                <a
                  href="https://drive.google.com/file/d/0B4nuwEMaEsnmazJvLXJUb0NuM1U/view?usp=sharing"
                  >powerpoint</a
                >,
                <a
                  href="https://drive.google.com/file/d/0B4nuwEMaEsnmTDBUWE96VHJndjg/view?usp=sharing"
                  >PDF</a
                >) /
                <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a> /
                <a
                  href="https://drive.google.com/file/d/1vg9Rb-kBntSTnTCzVgFlskkPXvTB_5aq/view?usp=sharing"
                  >code &amp; data</a
                >
                /
                <a
                  href="https://drive.google.com/file/d/11X5Zfjy7Q7oP_V2rtqy2f5-x9YgQUAFd/view?usp=sharing"
                  >kudos</a
                >
              
              <p>
                We present <strong>SIRFS</strong>, which can estimate shape,
                chromatic illumination, reflectance, and shading from a single
                image of an masked object.
              </p>
              <p>
                This paper subsumes our CVPR 2011, CVPR 2012, and ECCV 2012
                papers.
              </p>
            </div>
          </article>

          <article>
            
              <div class='one'>
                <img
                  src="images/ArbalaezCVPR2014.jpg"
                  alt="ArbalaezCVPR2014"
                  width="160"
                  height="120"
                  style="border-style: none"
                />
              </div>
              
              <div class="article-info"> 
              
                <a
                href="https://drive.google.com/file/d/1M0wijHY134F9ETBgO8mjeuKUSblTRLG0/view?usp=sharing"
              >
                <papertitle>Multiscale Combinatorial Grouping</papertitle>
              </a>
            <div>
                <a href="http://www.cs.berkeley.edu/~arbelaez/"
                  >Pablo Arbel&aacuteez</a
                >,
                <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset"
                  >Jordi Pont-Tuset</a
                >, <strong>Jonathan T. Barron</strong>,
                <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a
                >,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>CVPR</em>, 2014</div>
               
              <a
                href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/"
                >project page</a
              >
              /
              <a href="data/ArbelaezCVPR2014.bib">bibtex</a>
              <p>
                This paper is subsumed by
                <a href="#MCG_journal">our journal paper</a>.
              </p>
            </div>
          </article>

          <article >
            <div class="one">
              <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="flyspin">
                <img src="images/BarronICCV2013_160.gif" />
              </div>
              <img src="images/BarronICCV2013_160.jpg" />
            </div>

              
              <div class="article-info"> 
              <a
                href="https://drive.google.com/file/d/1shvItvx_8Sb8QNXhrOXkuRmx2618iwNJ/view?usp=sharing"
              >
                <papertitle
                  >Volumetric Semantic Segmentation using Pyramid Context
                  Features</papertitle
                >
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/~arbelaez/"
                  >Pablo Arbel&aacuteez</a
                >, <a href="http://big.lbl.gov/">Soile V. E. Ker&aumlnen</a>,
                <a href="http://www.lbl.gov/gsd/biggin.html">Mark D. Biggin</a>,
                 
                <a href="http://dwknowles.lbl.gov/">David W. Knowles</a>,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>ICCV</em>, 2013</div>
               
              <a
                href="https://drive.google.com/file/d/1htiLpMAcYLtuBthmAb4XHnOYxUbkfnqR/view?usp=sharing"
                >supplement</a
              >
              /
              <a
                href="https://drive.google.com/file/d/1qoYeFNa443myn2SfcdhmCsYBqE9xQrPD/view?usp=sharing"
                >poster</a
              >
              / <a href="data/BarronICCV2013.bib">bibtex</a> /
              <a href="http://www.youtube.com/watch?v=Y56-FcfnlVA&hd=1"
                >video 1</a
              >
              (or
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing"
                >mp4</a
              >) /
              <a href="http://www.youtube.com/watch?v=mvRoYuP6-l4&hd=1"
                >video 2</a
              >
              (or
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmZ1ZLaHdQYzAxNlU/view?usp=sharing"
                >mp4</a
              >) /
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmSF9YdWJjQmh4QW8/view?usp=sharing"
                >code &amp; data</a
              >
              <p>
                We present a technique for efficient per-voxel linear
                classification, which enables accurate and fast semantic
                segmentation of volumetric Drosophila imagery.
              </p>
            </div>
          </article>

          <article>
            
              <div class='one'>
                <img
                  src="images/3DSP_160.jpg"
                  alt="3DSP"
                  width="160"
                  height="120"
                  style="border-style: none"
                />
              </div>
              
              <div class="article-info"> 
              
                <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing"
                id="3DSP"
              >
                <papertitle>3D Self-Portraits</papertitle>
              </a>
              <div>
                <a href="http://www.hao-li.com/">Hao Li</a>,
                <a href="http://www.evouga.com/">Etienne Vouga</a>, Anton Gudym,
                <a href="http://www.cs.princeton.edu/~linjiel/">Linjie Luo</a>,
                <strong>Jonathan T. Barron</strong>, Gleb Gusev
              </div>
               
              <div><em>SIGGRAPH Asia</em>, 2013</div>
               
              <a href="http://www.youtube.com/watch?v=DmUkbZ0QMCA">video</a> /
              <a href="http://shapify.me/">shapify.me</a> /
              <a href="data/3DSP_siggraphAsia2013.bib">bibtex</a>
              <p>
                Our system allows users to create textured 3D models of
                themselves in arbitrary poses using only a single 3D sensor.
              </p>
            </div>
          </article>

          <article>
            <div class="one">
              <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="rgbd_anim">
                <img src="images/SceneSIRFS.gif" />
              </div>
              <img src="images/SceneSIRFS-still.jpg" />
            </div>

              
              <div class="article-info"> 
                <a
                href="https://drive.google.com/file/d/1snypSLhzC0jXCchJRsWpcDZ7Es5hDmXo/view?usp=sharing"
              >
                <papertitle
                  >Intrinsic Scene Properties from a Single RGB-D
                  Image</papertitle
                >
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>CVPR</em>, 2013 &nbsp
              <font color="red"><strong>(Oral Presentation)</strong></font></div>
               
              <a
                href="https://drive.google.com/file/d/1cLUw72WpgdZ_3TQAjJABdgywqjBfn_Mq/view?usp=sharing"
                >supplement</a
              >
              / <a href="data/BarronMalikCVPR2013.bib">bibtex</a> /
              <a
                href="http://techtalks.tv/talks/intrinsic-scene-properties-from-a-single-rgb-d-image/58614/"
                >talk</a
              >
              /
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmWW1CZGJPbi12R0k/view?usp=sharing"
                >keynote</a
              >
              (or
              <a
                href="https://drive.google.com/file/d/19q3EFf6GIb4UFcCN2DVU2jVKpxRj5kxf/view?usp=sharing"
                >powerpoint</a
              >,
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmMzQ4ZVp1SWdnVkk/view?usp=sharing"
                >PDF</a
              >) /
              <a
                href="https://drive.google.com/open?id=1ZbPScVA6Efqd-ESvojl92sw8K-82Xxry"
                >code &amp; data</a
              >
              <p>
                By embedding mixtures of shapes &amp; lights into a soft
                segmentation of an image, and by leveraging the output of the
                Kinect, we can extend SIRFS to scenes.
                 
                 TPAMI Journal version:
                <a
                  href="https://drive.google.com/file/d/1iQiUxZvjPPnb8rFCwXYesTgFSRk7mkAq/view?usp=sharing"
                  >version</a
                >
                / <a href="data/BarronMalikTPAMI2015B.bib">bibtex</a>
              </p>
            </div>
          </article>

          <article>
            
              <img class='one'
                src="images/Boundary.jpg"
                alt="Boundary_png"
                style="border-style: none"
              />
              
              <div class="article-info"> 
              
                <a
                href="https://drive.google.com/file/d/1H4YPovfrvcce3HGMEhidwU2l2fTcNR5y/view?usp=sharing"
              >
                <papertitle
                  >Boundary Cues for 3D Object Shape Recovery</papertitle
                >
              </a>
              <div>
                <a href="http://www.kevinkarsch.com/">Kevin Karsch</a>,
                <a href="http://web.engr.illinois.edu/~liao17/">Zicheng Liao</a>,
                <a href="http://web.engr.illinois.edu/~jjrock2/">Jason Rock</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.illinois.edu/homes/dhoiem/">Derek Hoiem</a>
              </div>
               
              <div><em>CVPR</em>, 2013</div>
               
              <a
                href="https://drive.google.com/file/d/0B4nuwEMaEsnmLUQ5SVJTcUZIYXc/view?usp=sharing"
                >supplement</a
              >
              / <a href="data/KarschCVPR2013.bib">bibtex</a>
              <p>
                Boundary cues (like occlusions and folds) can be used for shape
                reconstruction, which improves object recognition for humans and
                computers.
              </p>
            </div>
          </article>

          <article>
            <div class="one">
              <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="eccv12_anim">
                <img src="images/ECCV2012_small.gif" />
              </div>
              <img src="images/ECCV2012_still.jpg" />
            </div>

              
              
              <div class="article-info"> 
                <a
                href="https://drive.google.com/file/d/1NczR4pJ-s0YBjCe0rCevMt8IM5JPuUrc/view?usp=sharing"
              >
                <papertitle
                  >Color Constancy, Intrinsic Images, and Shape
                  Estimation</papertitle
                >
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>ECCV</em>, 2012</div>
               
              <a
                href="https://drive.google.com/file/d/1zuxhWZ3i6THvuRRBeE7dM_BJfDxO72Fq/view?usp=sharing"
                >supplement</a
              >
              / <a href="data/BarronMalikECCV2012.bib">bibtex</a> /
              <a
                href="https://drive.google.com/file/d/12x8mhqpFsA6p0u6ZQW-ieRKF8hlQBKKe/view?usp=sharing"
                >poster</a
              >
              /
              <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a>
              <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
            </div>
          </article>

          <article>
            
              <div class="one" style="height: 120px">
                <div class="two" onmouseout="animation_stop(this.id)" onmouseover="animation_start(this.id)" id="cvpr12_image" style="height: 120px">
                  <img
                    src="images/BarronCVPR2012_after.jpg"
                    style="border-style: none"
                  />
                </div>
                <img
                  src="images/BarronCVPR2012_before.jpg"
                  style="border-style: none"
                />
              </div>
              
              <div class="article-info"> 
              
                <a
                href="https://drive.google.com/file/d/17RfINbE2dr2EjXp9MtGO0MHJLQmQVhvT/view?usp=sharing"
              >
                <papertitle
                  >Shape, Albedo, and Illumination from a Single Image of an
                  Unknown Object</papertitle
                >
              </a>
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>CVPR</em>, 2012</div>
               
              <a
                href="https://drive.google.com/file/d/1Im_bUI42AP9VPoNtsjLajvtLRiwv39k3/view?usp=sharing"
                >supplement</a
              >
              / <a href="data/BarronMalikCVPR2012.bib">bibtex</a> /
              <a
                href="https://drive.google.com/file/d/1IAlSF4k3_CEL9dfbaMiNTFPBoEkLhsRl/view?usp=sharing"
                >poster</a
              >
              <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
            </div>
          </article>

          <article>
            
              <img
                src="images/B3DO.jpg"
                alt="b3do"
                width="160"
                style="border-style: none"
                class='one'
              />
              
              <div class="article-info"> 
              
                <a
                href="https://drive.google.com/file/d/1_S8EQyngbHQrB415o0XkQ4V9SMzdEgWT/view?usp=sharing"
              >
                <papertitle
                  >A Category-Level 3-D Object Dataset: Putting the Kinect to
                  Work</papertitle
                >
              </a>
              <div>
                <a href="http://www.eecs.berkeley.edu/%7Eallie/">Allison Janoch</a
                >, <a href="http://sergeykarayev.com/">Sergey Karayev</a>,
                <a href="http://www.eecs.berkeley.edu/%7Ejiayq/">Yangqing Jia</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/%7Emfritz/">Mario Fritz</a>,
                <a href="http://www.icsi.berkeley.edu/%7Esaenko/">Kate Saenko</a>,
                <a href="http://www.eecs.berkeley.edu/%7Etrevor/"
                  >Trevor Darrell</a
                >
              </div>
               
              <div><em>ICCV 3DRR Workshop</em>, 2011</div>
               
              <a href="data/B3DO_ICCV_2011.bib">bibtex</a> /
              <a
                href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing"
                >"smoothing" code</a
              >
              <p>
                We present a large RGB-D dataset of indoor scenes and
                investigate ways to improve object detection using depth
                information.
              </p>
            </div>
          </article>

          <article>
            
              <img class='one'
                src="images/safs.jpg"
                alt="safs_small"
                width="160"
                height="160"
                style="border-style: none"
              />
              
              <div class="article-info"> 
                <a
                href="https://drive.google.com/file/d/1EZTOO5xezLYcyIFgAzs4KuZFLbTcwTDH/view?usp=sharing"
              >
                <papertitle
                  >High-Frequency Shape and Albedo from Shading using Natural
                  Image Statistics</papertitle
                >
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>CVPR</em>, 2011</div>
               
              <a href="data/BarronMalikCVPR2011.bib">bibtex</a>
              <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
            </div>
          </article>

          <article>
            
              <img class='one'
                src="images/fast_texture.jpg"
                alt="fast-texture"
                width="160"
                height="160"
              />
              
              <div class="article-info"> 
                <a
                href="https://drive.google.com/file/d/1rc05NatkQVmUDlGCAYcHSrvAzTpU9knT/view?usp=sharing"
              >
                <papertitle
                  >Discovering Efficiency in Coarse-To-Fine Texture
                  Classification</papertitle
                >
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
              </div>
               
              <div><em>Technical Report</em>, 2010</div>
               
              <a href="data/BarronTR2010.bib">bibtex</a>
              <p>
                A model and feature representation that allows for sub-linear
                coarse-to-fine semantic segmentation.
              </p>
            </div>
          </article>

          <article>
            
              <img src="images/prl.jpg" alt="prl" width="160" height="160" class='one'/>
              
              <div class="article-info"> 
                <a
                href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing"
              >
                <papertitle>Parallelizing Reinforcement Learning</papertitle>
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>,
                <a href="http://www.cs.berkeley.edu/~nickjhay/"
                  >Nicholas J. Hay</a
                >
              </div>
               
              <div><em>Technical Report</em>, 2009</div>
               
              <a href="data/BarronPRL2009.bib">bibtex</a>
              <p>
                Markov Decision Problems which lie in a low-dimensional latent
                space can be decomposed, allowing modified RL algorithms to run
                orders of magnitude faster in parallel.
              </p>
            </div>
          </article>

          <article>
            
              <img class='one'
                src="images/bd_promo.jpg"
                alt="blind-date"
                width="160"
                height="160"
              />
              
              <div class="article-info"> 
                <a
                href="https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing"
              >
                <papertitle
                  >Blind Date: Using Proper Motions to Determine the Ages of
                  Historical Images</papertitle
                >
              </a>
               
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>,
                <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>,
                <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
              </div>
               
              <div><em>The Astronomical Journal</em>, 136, 2008</div>
              <p>
                Using the relative motions of stars we can accurately estimate
                the date of origin of historical astronomical images.
              </p>
            </div>
          </article>

          <article>
            
              <img
                src="images/clean_promo.jpg"
                alt="clean-usnob"
                width="160"
                height="160"
                class='one'
              />
              
              <div class="article-info"> 
              
                <a
                href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing"
              >
                <papertitle
                  >Cleaning the USNO-B Catalog Through Automatic Detection of
                  Optical Artifacts</papertitle
                >
              </a>
              <div>
                <strong>Jonathan T. Barron</strong>,
                <a href="http://stumm.ca/">Christopher Stumm</a>,
                <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>,
                <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>,
                <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
              </div>
               
              <div><em>The Astronomical Journal</em>, 135, 2008</div>
              <p>
                We use computer vision techniques to identify and remove
                diffraction spikes and reflection halos in the USNO-B Catalog.
              </p>
              <p>
                In use at <a href="http://www.astrometry.net">Astrometry.net</a>
              </p>
            </div>
          
        </article>
      </div>

      <table
        width="100%"
        align="center"
        border="0"
        cellspacing="0"
        cellpadding="20"
      >
        <tbody>
          <tr>
            <td>
              <h2 id='service-link'>Service</h2>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>
          <tr>
            <td style="padding: 20px; width: 25%; vertical-align: middle">
              <img src="images/cvf.jpg" />
            </td>
            <td width="75%" valign="center">
              <a href="http://cvpr2021.thecvf.com/area-chairs"
                >Area Chair, CVPR 2021</a
              >
                
              <a href="http://cvpr2019.thecvf.com/area_chairs"
                >Area Chair, CVPR 2019</a
              >
                
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs"
                >Area Chair, CVPR 2018</a
              >
            </td>
          </tr>
          <tr>
            <td style="padding: 20px; width: 25%; vertical-align: middle">
              <img src="images/cs188.jpg" alt="cs188" />
            </td>
            <td width="75%" valign="center">
              <a
                href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html"
                >Graduate Student Instructor, CS188 Spring 2011</a
              >
               
               
              <a
                href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html"
                >Graduate Student Instructor, CS188 Fall 2010</a
              >
               
               
              <a href="http://aima.cs.berkeley.edu/"
                >Figures, "Artificial Intelligence: A Modern Approach", 3rd
                Edition</a
              >
            </td>
          </tr>
        </tbody>
      </table>
      <table
        style="
          width: 100%;
          border: 0px;
          border-spacing: 0px;
          border-collapse: separate;
          margin-right: auto;
          margin-left: auto;
        "
      >
        <tbody>
          <tr>
            <td style="padding: 0px">
              <br />
              <p style="text-align: right; font-size: small">
                Feel free to steal this website's
                <a href="https://github.com/jonbarron/jonbarron_website"
                  >source code</a
                >, just add a link back to my website.
                <strong>Do not</strong> scrape the HTML from the deployed
                instance of this website at http://jonbarron.info, as it
                includes analytics tags that you do not want on your own website
                &mdash; use the github code instead. If you'd like your new page
                linked to from here, submit a pull request adding yourself.
                <a href="https://www.cs.ubc.ca/~pbateni/">&#10025;</a>
                <a href="https://aliosmanulusoy.github.io/">&#10025;</a>
                <a href="https://cs.stanford.edu/~poole/">&#10025;</a>
                <a href="http://www.cs.berkeley.edu/~akar/">&#10025;</a>
                <a href="http://www.eecs.berkeley.edu/~biancolin">&#10025;</a>
                <a href="http://www.rossgirshick.info/">&#10025;</a>
                <a href="http://www.cs.cmu.edu/~igkioule/">&#10025;</a>
                <a href="http://kelvinxu.github.io/">&#10025;</a>
                <a href="http://imagine.enpc.fr/~groueixt/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~cbfinn/">&#10025;</a>
                <a href="http://disi.unitn.it/~nabi/">&#10025;</a>
                <a href="http://changyeobshin.com/">&#10025;</a>
                <a href="https://mbanani.github.io/">&#10025;</a>
                <a href="https://aseembits93.github.io">&#10025;</a>
                <a href="http://fuwei.us/">&#10025;</a>
                <a href="http://www-bcf.usc.edu/~iacopoma/">&#10025;</a>
                <a href="https://lorisbaz.github.io/">&#10025;</a>
                <a href="https://dplarson.info/">&#10025;</a>
                <a href="http://chapiro.net/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~vitchyr/"
                  >&#10025;</a
                >
                <a href="https://people.eecs.berkeley.edu/~kellman/"
                  >&#10025;</a
                >
                <a href="http://www0.cs.ucl.ac.uk/staff/C.Godard/">&#10025;</a>
                <a href="http://www.cs.toronto.edu/~byang/">&#10025;</a>
                <a href="http://people.kyb.tuebingen.mpg.de/harmeling/"
                  >&#10025;</a
                >
                <a href="https://prakashmurali.bitbucket.io/">&#10025;</a>
                <a href="http://www.cs.bham.ac.uk/~exa371/">&#10025;</a>
                <a href="http://prosello.com/">&#10025;</a>
                <a href="http://www.ee.ucr.edu/~nmithun/">&#10025;</a>
                <a href="https://rmullapudi.bitbucket.io/">&#10025;</a>
                <a href="http://www.briangauch.com/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~coline/">&#10025;</a>
                <a href="https://www.andrew.cmu.edu/user/sjayasur/website.html"
                  >&#10025;</a
                >
                <a href="http://www.eecs.berkeley.edu/~rakelly/">&#10025;</a>
                <a href="https://gkioxari.github.io/">&#10025;</a>
                <a href="http://ai.stanford.edu/~hsong/">&#10025;</a>
                <a href="http://www.ee.ucr.edu/~mbappy/">&#10025;</a>
                <a href="http://adithyamurali.com/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~khoury/">&#10025;</a>
                <a href="https://prashanthtk.github.io/">&#10025;</a>
                <a href="http://tomhenighan.com/">&#10025;</a>
                <a href="http://mbchang.github.io/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~haarnoja/"
                  >&#10025;</a
                >
                <a href="http://web.stanford.edu/~sfort1/">&#10025;</a>
                <a href="http://www.arkin.xyz/">&#10025;</a>
                <a href="http://i-am-karan-singh.github.io/">&#10025;</a>
                <a href="https://pxlong.github.io/">&#10025;</a>
                <a href="https://dheeraj2444.github.io/">&#10025;</a>
                <a href="https://fabienbaradel.github.io/">&#10025;</a>
                <a href="https://ankitdhall.github.io/">&#10025;</a>
                <a href="http://nafiz.ml/">&#10025;</a>
                <a href="http://www.cs.cmu.edu/~aayushb">&#10025;</a>
                <a href="http://bjornstenger.github.io/">&#10025;</a>
                <a href="http://users.eecs.northwestern.edu/~mif365/"
                  >&#10025;</a
                >
                <a href="https://www.macs.hw.ac.uk/~ic14/">&#10025;</a>
                <a href="https://ai.stanford.edu/~kaidicao/">&#10025;</a>
                <a href="http://hengfan.byethost7.com/">&#10025;</a>
                <a href="https://reyhaneaskari.github.io/">&#10025;</a>
                <a href="https://tianheyu927.github.io/">&#10025;</a>
                <a href="http://people.csail.mit.edu/janner/">&#10025;</a>
                <a href="http://www.sjoerdvansteenkiste.com/">&#10025;</a>
                <a href="http://joaoloula.github.io/">&#10025;</a>
                <a href="https://bhairavmehta95.github.io/">&#10025;</a>
                <a href="https://palmieri.github.io/">&#10025;</a>
                <a href="https://psuriana.github.io/">&#10025;</a>
                <a href="http://yushi2.web.engr.illinois.edu/">&#10025;</a>
                <a href="http://ruthcfong.github.io/">&#10025;</a>
                <a href="https://shraman-rc.github.io/">&#10025;</a>
                <a href="http://rahulgarg.com/">&#10025;</a>
                <a href="http://www.cs.cmu.edu/~inigam/">&#10025;</a>
                <a href="http://djstrouse.com/">&#10025;</a>
                <a href="https://lekhamohan.github.io/">&#10025;</a>
                <a href="https://avijit9.github.io/">&#10025;</a>
                <a href="http://www.seas.ucla.edu/~sahba/">&#10025;</a>
                <a href="https://pages.jh.edu/~falambe1/">&#10025;</a>
                <a href="http://www.dcc.fc.up.pt/~vitor.cerqueira/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~bmild/">&#10025;</a>
                <a href="https://web.eecs.umich.edu/~subh/">&#10025;</a>
                <a href="http://www.cs.utexas.edu/~pgoyal/">&#10025;</a>
                <a href="http://www.eecs.wsu.edu/~fchowdhu/">&#10025;</a>
                <a href="https://aarzchan.github.io/">&#10025;</a>
                <a href="https://www.seas.upenn.edu/~oleh/">&#10025;</a>
                <a href="http://shamak.github.io/">&#10025;</a>
                <a href="http://jianfeng.us/">&#10025;</a>
                <a href="https://pulkitkumar95.github.io/">&#10025;</a>
                <a href="https://epiception.github.io/">&#10025;</a>
                <a href="https://weimengpu.github.io/">&#10025;</a>
                <a href="http://users.ices.utexas.edu/~faraz/">&#10025;</a>
                <a href="https://vitorgodeiro.github.io/">&#10025;</a>
                <a href="http://cgm.technion.ac.il/people/Roey/">&#10025;</a>
                <a href="https://mancinimassimiliano.github.io/">&#10025;</a>
                <a href="https://roshanjrajan.me/">&#10025;</a>
                <a href="http://irc.cs.sdu.edu.cn/~qingnan/">&#10025;</a>
                <a href="http://individual.utoronto.ca/yuenj/">&#10025;</a>
                <a href="https://akhileshgotmare.github.io/">&#10025;</a>
                <a href="http://vllab.ucmerced.edu/nakul/">&#10025;</a>
                <a href="https://hasibzunair.github.io/">&#10025;</a>
                <a href="http://dalezhou.com/">&#10025;</a>
                <a href="https://abhoi.github.io/">&#10025;</a>
                <a href="https://www.cse.unr.edu/~jyi/">&#10025;</a>
                <a href="http://www.liuzhaolun.com/">&#10025;</a>
                <a href="https://abhisheknaik.me/">&#10025;</a>
                <a href="https://cfernandezlab.github.io/">&#10025;</a>
                <a href="https://aasharma90.github.io/">&#10025;</a>
                <a href="https://kdizon.github.io/">&#10025;</a>
                <a href="https://www.cse.wustl.edu/~zhihao.xia/">&#10025;</a>
                <a href="http://mmozes.net/">&#10025;</a>
                <a href="https://kpertsch.github.io/">&#10025;</a>
                <a href="http://xiatianpei.com/">&#10025;</a>
                <a href="https://nsrishankar.github.io/">&#10025;</a>
                <a href="http://xujuefei.com/">&#10025;</a>
                <a href="https://www.cs.rochester.edu/u/lchen63/">&#10025;</a>
                <a href="http://deyachatterjee.github.io/">&#10025;</a>
                <a href="http://hossein1387.github.io/index.html">&#10025;</a>
                <a href="https://zx007zls.github.io/">&#10025;</a>
                <a href="http://people.eecs.berkeley.edu/~nol/">&#10025;</a>
                <a href="http://www.cs.princeton.edu/~jw60/">&#10025;</a>
                <a href="https://cseweb.ucsd.edu/~owen/">&#10025;</a>
                <a href="https://subhajitchaudhury.github.io/">&#10025;</a>
                <a href="https://sandarshp.github.io/">&#10025;</a>
                <a href="https://medhini.github.io/">&#10025;</a>
                <a href="http://cindyxinyiwang.github.io/">&#10025;</a>
                <a href="https://lasirenashann.github.io/">&#10025;</a>
                <a href="http://ambuj.se/">&#10025;</a>
                <a href="http://kylehsu.me/">&#10025;</a>
                <a href="https://ujjwal95.github.io/">&#10025;</a>
                <a href="https://aditya5558.github.io/">&#10025;</a>
                <a href="http://www.majumderb.com/">&#10025;</a>
                <a href="http://ylqiao.net/">&#10025;</a>
                <a href="https://xiaochunliu.github.io/">&#10025;</a>
                <a href="https://dhawgupta.github.io/">&#10025;</a>
                <a href="http://cliu.info/">&#10025;</a>
                <a href="https://taochenshh.github.io/">&#10025;</a>
                <a href="http://www-scf.usc.edu/~ayushj/">&#10025;</a>
                <a href="https://zexuehe.github.io/">&#10025;</a>
                <a href="https://ofkar.github.io/">&#10025;</a>
                <a href="https://amir-arsalan.github.io/">&#10025;</a>
                <a href="https://vinamrabenara.github.io/">&#10025;</a>
                <a href="https://likojack.bitbucket.io/">&#10025;</a>
                <a href="http://www-personal.umich.edu/~zeyu/">&#10025;</a>
                <a href="https://utkarshojha.github.io/">&#10025;</a>
                <a href="https://fahmidmorshed.github.io/">&#10025;</a>
                <a href="https://www.cs.ubc.ca/~setarehc/">&#10025;</a>
                <a href="http://viveksolanki.com/">&#10025;</a>
                <a href="http://webdiis.unizar.es/~alcolea/">&#10025;</a>
                <a href="http://www.songyaojiang.com/">&#10025;</a>
                <a href="https://www.csee.umbc.edu/~bhp1/">&#10025;</a>
                <a href="http://acl.mit.edu/people">&#10025;</a>
                <a href="https://vignanv.com/">&#10025;</a>
                <a href="https://liuyicun.me/">&#10025;</a>
                <a href="https://heyuanmingong.github.io/">&#10025;</a>
                <a href="https://abhishekaich27.github.io/">&#10025;</a>
                <a href="https://www.cs.ubc.ca/~saeidnp/">&#10025;</a>
                <a href="https://uuujf.github.io/">&#10025;</a>
                <a href="https://csjcai.github.io/">&#10025;</a>
                <a href="https://iphyer.github.io/Mingren_Website/">&#10025;</a>
                <a href="https://binzhubz.github.io/">&#10025;</a>
                <a href="https://steinar.dev/">&#10025;</a>
                <a href="https://tnq177.github.io/">&#10025;</a>
                <a href="https://jonathan-schwarz.github.io/">&#10025;</a>
                <a href="http://www.ronnieclark.co.uk/">&#10025;</a>
                <a href="http://www.aparnadhinakaran.com/">&#10025;</a>
                <a href="https://zjysteven.github.io/">&#10025;</a>
                <a href="http://aakash30jan.github.io/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~shelhamer/"
                  >&#10025;</a
                >
                <a href="https://junaidcs032.github.io/">&#10025;</a>
                <a href="https://liangxuy.github.io/">&#10025;</a>
                <a href="http://www.public.asu.edu/~ssarka18/">&#10025;</a>
                <a href="https://www.cs.toronto.edu/~jennachoi/">&#10025;</a>
                <a href="https://amankhullar.github.io/">&#10025;</a>
                <a href="https://vijayvee.github.io/">&#10025;</a>
                <a href="https://anmolgoel.dev/">&#10025;</a>
                <a href="https://chanh.ee/">&#10025;</a>
                <a href="https://jefflai108.github.io/">&#10025;</a>
                <a href="https://pvskand.github.io/">&#10025;</a>
                <a href="https://ariostgx.github.io/website/">&#10025;</a>
                <a href="https://www.cs.utexas.edu/~shreyd/">&#10025;</a>
                <a href="https://www.cedrick.ai/">&#10025;</a>
                <a href="https://onlytailei.github.io/">&#10025;</a>
                <a href="https://people.cs.vt.edu/liminyang/">&#10025;</a>
                <a href="https://alexander-kirillov.github.io/">&#10025;</a>
                <a href="https://sites.cs.ucsb.edu/~yanju/">&#10025;</a>
                <a href="https://www.cct.lsu.edu/~cliu/">&#10025;</a>
                <a href="https://sid2697.github.io/">&#10025;</a>
                <a href="https://brendonjeromebutler.com/">&#10025;</a>
                <a href="https://bthananjeyan.github.io/">&#10025;</a>
                <a href="https://leonidk.com/">&#10025;</a>
                <a href="http://bopeng.space/">&#10025;</a>
                <a href="https://gautamigolani.github.io/">&#10025;</a>
                <a href="https://rohitrango.github.io/">&#10025;</a>
                <a href="https://prithv1.github.io/">&#10025;</a>
                <a href="https://www.idiap.ch/~tpereira/">&#10025;</a>
                <a href="https://felipefelixarias.github.io/">&#10025;</a>
                <a href="https://sophieschau.github.io/">&#10025;</a>
                <a href="https://wensun.github.io/">&#10025;</a>
                <a href="http://cs.umanitoba.ca/~kumarkm/">&#10025;</a>
                <a href="https://sourav-roni.github.io/">&#10025;</a>
                <a href="https://parskatt.github.io/">&#10025;</a>
                <a href="https://rauldiaz.github.io/">&#10025;</a>
                <a href="https://uzeful.github.io/">&#10025;</a>
                <a href="https://roeiherz.github.io/">&#10025;</a>
                <a href="http://relh.net/">&#10025;</a>
                <a href="https://ars-ashuha.ru/">&#10025;</a>
                <a href="https://prieuredesion.github.io/">&#10025;</a>
                <a href="https://tsujuifu.github.io/">&#10025;</a>
                <a href="https://pranoy-k.github.io/website/">&#10025;</a>
                <a href="https://xiaoleiz.github.io/">&#10025;</a>
                <a href="https://users.ece.cmu.edu/~bahn/">&#10025;</a>
                <a href="http://eracah.github.io/">&#10025;</a>
                <a href="https://adityakusupati.github.io/">&#10025;</a>
                <a href="https://agadetsky.github.io/">&#10025;</a>
                <a href="https://coh1211.github.io/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~kevinlin/"
                  >&#10025;</a
                >
                <a href="https://bucherb.github.io/">&#10025;</a>
                <a href="https://naman-ntc.github.io/">&#10025;</a>
                <a href="https://nicolay-r.github.io/">&#10025;</a>
                <a href="https://sakshambassi.github.io/">&#10025;</a>
                <a href="https://maheshmohanmr.github.io/publications/"
                  >&#10025;</a
                >
                <a href="http://byang.org/">&#10025;</a>
                <a href="https://sylqiu.github.io/">&#10025;</a>
                <a href="https://www.cc.gatech.edu/~sfoley30/">&#10025;</a>
                <a href="https://crockwell.github.io/">&#10025;</a>
                <a href="https://kritikalcoder.github.io/">&#10025;</a>
                <a href="https://isrugeek.github.io/">&#10025;</a>
                <a href="https://wei-ying.net/">&#10025;</a>
                <a href="http://people.csail.mit.edu/liuyingcheng/">&#10025;</a>
                <a href="https://itsreddy.github.io/">&#10025;</a>
                <a href="https://aditimavalankar.github.io">&#10025;</a>
                <a href="https://www.cs.ubc.ca/~zhenanf/">&#10025;</a>
                <a href="https://trinkle23897.github.io/cv/">&#10025;</a>
                <a href="https://tgangwani.github.io/">&#10025;</a>
                <a href="https://hannahlawrence.github.io/">&#10025;</a>
                <a href="http://bingxu.tech/">&#10025;</a>
                <a href="http://apjacob.me/">&#10025;</a>
                <a href="https://www.ee.iitb.ac.in/student/~krishnasubramani/"
                  >&#10025;</a
                >
                <a href="https://gowthamkuntumalla.github.io/">&#10025;</a>
                <a href="https://dkkim93.github.io/">&#10025;</a>
                <a href="https://chaitanyamalaviya.github.io/">&#10025;</a>
                <a href="https://daochenw.github.io/">&#10025;</a>
                <a href="https://acvictor.github.io/">&#10025;</a>
                <a href="https://tarund1996.github.io/">&#10025;</a>
                <a href="http://www-personal.umich.edu/~belz/">&#10025;</a>
                <a href="http://lauredelisle.com/">&#10025;</a>
                <a href="https://tsly123.github.io/">&#10025;</a>
                <a href="https://vanditg.github.io/">&#10025;</a>
                <a href="https://nhoma.github.io/">&#10025;</a>
                <a href="http://www.pedro.opinheiro.com/">&#10025;</a>
                <a href="https://suvan.sh/">&#10025;</a>
                <a href="https://www.diptanu.com/">&#10025;</a>
                <a href="https://yimengli46.github.io/">&#10025;</a>
                <a href="https://gauravparmar.com/">&#10025;</a>
                <a href="https://cchoquette.github.io/">&#10025;</a>
                <a href="https://psyche-mia.github.io/">&#10025;</a>
                <a href="https://mvp18.github.io/">&#10025;</a>
                <a href="https://markfzp.github.io/">&#10025;</a>
                <a href="https://theultramarine19.github.io/">&#10025;</a>
                <a href="https://sairajk.github.io/">&#10025;</a>
                <a href="https://paritoshparmar.github.io/">&#10025;</a>
                <a href="https://purvak-l.github.io/">&#10025;</a>
                <a href="https://davideabati.info/">&#10025;</a>
                <a href="https://ethanperez.net/">&#10025;</a>
                <a href="http://mdai.me/">&#10025;</a>
                <a href="https://nitish-nagesh.github.io/">&#10025;</a>
                <a href="https://duroz.github.io/">&#10025;</a>
                <a href="http://people.uncw.edu/dogang/">&#10025;</a>
                <a href="http://hal9000.space">&#10025;</a>
                <a href="http://web.stanford.edu/~yjiang05/">&#10025;</a>
                <a href="https://people.eecs.berkeley.edu/~pratul/">&#10025;</a>
                <a href="http://linuxdeveloper.io">&#10025;</a>
                <a href="http://umiacs.umd.edu/~kdbrant">&#10025;</a>
                <a href="https://adityassrana.github.io/blog/about">&#10025;</a>
                <a href="https://mpalrocks.github.io/">&#10025;</a>
                <a href="https://victor7246.github.io/">&#10025;</a>
                <a href="https://www.haozhu-wang.com/">&#10025;</a>
                <a href="https://rajat499.github.io/">&#10025;</a>
                <a href="https://senya-ashukha.github.io/">&#10025;</a>
                <a href="https://qiminchen.github.io/">&#10025;</a>
                <a href="https://mark12ding.github.io/">&#10025;</a>
                <a href="https://zsb87.github.io/">&#10025;</a>
                <a href="https://yashbhalgat.github.io/">&#10025;</a>
                <a href="https://r0cketr1kky.github.io/">&#10025;</a>
                <a href="https://jaydeepborkar.github.io/">&#10025;</a>
                <a href="https://lidq92.github.io/">&#10025;</a>
                <a href="https://statho.github.io/">&#10025;</a>
                <a href="https://ankit-kaul.github.io/">&#10025;</a>
                <a href="https://jlidard.github.io/">&#10025;</a>
                <a href="http://harshpanwar.me/">&#10025;</a>
                <a href="https://c0ldstudy.github.io/about/">&#10025;</a>
                <a href="https://pro-vider.github.io/personal-website/"
                  >&#10025;</a
                >
                <a href="http://avinashpaliwal.com/">&#10025;</a>
                <a href="https://soham-official.github.io/portfolio/"
                  >&#10025;</a
                >
                <a href="https://cdmdc.github.io/">&#10025;</a>
                <a href="http://www.wisdom.weizmann.ac.il/~/assafsho/"
                  >&#10025;</a
                >
                <a href="https://yotamnitzan.github.io/">&#10025;</a>
                <a href="https://www.zijianwang.me/">&#10025;</a>
                <a href="https://people.rit.edu/hv8322/">&#10025;</a>
                <a href="https://ilyac.info/">&#10025;</a>
                <a href="https://tamarott.github.io/">&#10025;</a>
                <a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/"
                  >&#10025;</a
                >
                <a href="https://stanford.edu/~amywang1/">&#10025;</a>
                <a href="https://g1910.github.io/">&#10025;</a>
                <a href="http://ozzie00.github.io/">&#10025;</a>
                <a href="http://xiomarag.github.io">&#10025;</a>
                <br />
                Also, consider using
                <a href="https://leonidk.com/">Leonid Keselman</a>'s
                <a href="https://github.com/leonidk/new_website">Jekyll fork</a>
                of this page.
              </p>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
    <script src="script.js"></script> 
  </body>
</html>
